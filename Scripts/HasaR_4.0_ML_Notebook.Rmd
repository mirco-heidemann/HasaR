---
title: "HasaR - Hagelschadenschaetzung anhand Radardaten"
subtitle: "ML Approach"
author: "Mirco Heidemann"
date: "June 2022"
output:
  pdf_document: default
---
Die Modellierung besteht aus zwei Teilen (bedingte Wahrscheinlichkeit):
Im ersten Teil wird unterschieden, ob ein Schaden eintritt (Schaden ja/nein)
  - Logistische Regression

Tritt ein Schaden ein, so wird im zweiten Teil der erwartete Schaden
pro Gitterzellen mit einer Gamma likelihood (link=log) modelliert.
- GLM mit Gamma Verteilung
- GLM mit Poisson Verteilung fuer die geschaetzte Anzahl Schaeden

Version: HasaR 4.0 with a ML Approach
---

**Funktionen und Packages laden**
RMarkdown files are a special case, as they work slightly differently to .R files in terms of file paths, i.e. they behave like mini projects of their own, where the default working directory is where the Rmd file is saved. To save RMarkdown files in a basic folder structure set up, it’s recommended to use the [here package](https://github.com/jennybc/here_here) and its workflow.

```{r eval = FALSE, warning = FALSE}
## Funktionen und Packages laden
library(MASS)
library(dplyr)
library(tidyverse)
library(here)
library(caret)
library(NeuralNetTools)

pth_func <- here("Data/rfunctions/")
pth_rdata <- here("Data/rdata/")

## Gegitterte Schadendaten laden: grid.data
load(here(pth_rdata, '2022_Lossevents_Grid.Rdata'))
```

Daten laden und aufbereiten:
```{r eval = FALSE, warning = FALSE}
grid_data <- grid.data %>%
  mutate(nonzero = ifelse(schad.SchadSum > 0, 1, 0),
         fact_nonzero = as.factor(nonzero))
```

Examine dataset
Balancing? fact_nonzero: much more 0's
```{r}
summary(grid_data)
# with(grid_data, plot(grid_data, fact_nonzero, col = fact_nonzero))
```

The function *createDataPartition* can be used to create a stratified random sample of the data into training and test sets. Create a 80/20% split into training and testing data.
```{r}
trainIndex <- createDataPartition(grid_data$fact_nonzero, p = .8, 
                                  list = FALSE, 
                                  times = 1)

dat_train <- grid_data[trainIndex,]
dat_test  <- grid_data[-trainIndex,]

cat('Train rows: ', nrow(dat_train),"\n",
    'Test rows: ', nrow(dat_test),
    sep="")
```
By default, simple bootstrap resampling is used in the createDataPartition algorithm. Others are available, such as repeated K-fold cross-validation, leave-one-out etc. The function trainControl can be used to specifiy the type of resampling.

The first two arguments to train are the predictor and outcome data objects, respectively. The third argument, method, specifies the type of model (see train Model List or train Models By Tag). To illustrate, we will fit a boosted tree model via the gbm package. The basic syntax for fitting this model using repeated cross-validation is shown below:

See: https://topepo.github.io/caret/model-training-and-tuning.html#an-example
```{r warning = FALSE}
## 10-fold Cross Validation
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
```

```{r}
## Logistic regression
logitFit <- train(fact_nonzero ~ Anzahl + VersSumme + poh + meshs,
               data = dat_train,
               method = "glm",
               family = "binomial",
               trControl = fitControl)

## Boosted Random Forest
gbmFit <- train(fact_nonzero ~ Anzahl + VersSumme + poh + meshs,
                data = dat_train,
                method = "gbm",
                trControl = fitControl,
                verbose = FALSE)

## Neural Network
nnetFit <- train(fact_nonzero ~ Anzahl + VersSumme + poh + meshs,
               data = dat_train,
               method = "nnet",
               trControl = fitControl,
               # tuneGrid = data.frame(size = 2, decay = 0),
               trace = FALSE)
```

The plot function can be used to examine the relationship between the estimates of performance and the tuning parameters. For example, a simple invokation of the function shows the results for the first performance measure:
```{r}
trellis.par.set(caretTheme())
plot(nnetFit)
plot(gbmFit)
```
Variable importance
```{r}
varImp(logitFit)
varImp(nnetFit)

plot(varImp(object=logitFit),main="Logistic Regression - Variable Importance")
plot(varImp(object=nnetFit),main="Neural Network - Variable Importance")
```

Test set accuracy
```{r}
### predict on test dataset
logitFit_pred <- predict(logitFit, dat_test)
nnetFit_pred <- predict(nnetFit, dat_test)

##results
confusionMatrix(logitFit_pred, dat_test$fact_nonzero)
confusionMatrix(nnetFit_pred, dat_test$fact_nonzero)

table(dat_test$fact_nonzero, nnetFit_pred)
```

Plot Neural Network Model
(The B's stand for Bias)
```{r}
plotnet(nnetFit$finalModel)
```
See more about Neural Network: http://sebastianderi.com/aexam/hld_MODEL_neural.html

See more about modelling with caret: See: https://remiller1450.github.io/s230f19/caret2.html


Modelling loss with Neural Network Regression for those cells where a loss is predicted

Regression using Artificial Neural Networks
Why do we need to use Artificial Neural Networks for Regression instead of simply using Linear Regression?
The purpose of using Artificial Neural Networks for Regression over Linear Regression is that the linear regression can only learn the linear relationship between the features and target and therefore cannot learn the complex non-linear relationship. In order to learn the complex non-linear relationship between the features and target, we are in need of other techniques. One of those techniques is to use Artificial Neural Networks. Artificial Neural Networks have the ability to learn the complex relationship between the features and target due to the presence of activation function in each layer. Let’s look at what are Artificial Neural Networks and how do they work.
```{r}
nnetFit_loss <- train(schad.SchadSum ~ Anzahl + VersSumme + poh + meshs,
               data = subset(dat_train, nonzero == 1),
               method = "nnet",
               trControl = fitControl,
               # tuneGrid = data.frame(size = 2, decay = 0),
               trace = FALSE)
```

